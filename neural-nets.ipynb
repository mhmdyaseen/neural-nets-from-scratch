{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "4o3JxUUa2hmB"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_mnist():\n",
        "    from tensorflow.keras.datasets import mnist\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    X_train = X_train.reshape(-1, 28*28).astype(np.float32) / 255.0\n",
        "    X_test = X_test.reshape(-1, 28*28).astype(np.float32) / 255.0\n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "metadata": {
        "id": "7AwzhM7I26Xi"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_test, y_test = load_mnist()"
      ],
      "metadata": {
        "id": "zN9r0DTh2wua"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sgH6Tbn3L3X",
        "outputId": "d9844660-f1df-439c-ae81-c185375b3d08"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
              " array([5, 0, 4, ..., 5, 6, 8], dtype=uint8),\n",
              " array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
              " array([7, 2, 1, ..., 4, 5, 6], dtype=uint8))"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#one-hot encode labels\n",
        "def one_hot(y, num_classes):\n",
        "    return np.eye(num_classes)[y]"
      ],
      "metadata": {
        "id": "s1V3R5tp3dgl"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "VPHfhCeR4uSm"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    return -np.mean(np.sum(y_true * np.log(y_pred + 1e-10), axis=1))  # Add small epsilon to avoid log(0)"
      ],
      "metadata": {
        "id": "kzM6_5iQ5ZsE"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.b1 = np.zeros(hidden_size)\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "        self.b2 = np.zeros(output_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = relu(self.z1)\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = softmax(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def backward(self, X, y_true, y_pred):\n",
        "        m = X.shape[0]  #no of samples\n",
        "\n",
        "        #output layer gradient\n",
        "        dZ2 = y_pred - y_true\n",
        "        dW2 = np.dot(self.a1.T, dZ2) / m\n",
        "        db2 = np.mean(dZ2, axis=0)\n",
        "\n",
        "        #hidden layer gradient\n",
        "        dA1 = np.dot(dZ2, self.W2.T)\n",
        "        dZ1 = dA1 * (self.a1 > 0)  #ReLU derivative\n",
        "        dW1 = np.dot(X.T, dZ1) / m\n",
        "        db1 = np.mean(dZ1, axis=0)\n",
        "\n",
        "        return dW1, db1, dW2, db2\n",
        "\n",
        "    def update_parameters(self, dW1, db1, dW2, db2, learning_rate):\n",
        "        self.W1 -= learning_rate * dW1\n",
        "        self.b1 -= learning_rate * db1\n",
        "        self.W2 -= learning_rate * dW2\n",
        "        self.b2 -= learning_rate * db2"
      ],
      "metadata": {
        "id": "q7kRvB_e57-a"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#one-hot encode labels\n",
        "y_train_one_hot = one_hot(y_train, 10)\n",
        "y_test_one_hot = one_hot(y_test, 10)"
      ],
      "metadata": {
        "id": "xSyZ8WXP7U1f"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    #shuffle data\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train_shuffled = X_train[indices]\n",
        "    y_train_shuffled = y_train_one_hot[indices]\n",
        "\n",
        "    #mini-batch training\n",
        "    for i in range(0, X_train.shape[0], batch_size):\n",
        "        X_batch = X_train_shuffled[i:i+batch_size]\n",
        "        y_batch = y_train_shuffled[i:i+batch_size]\n",
        "\n",
        "        #forward feed\n",
        "        y_pred = nn.forward(X_batch)\n",
        "\n",
        "        loss = cross_entropy_loss(y_batch, y_pred)\n",
        "\n",
        "        #backward prop\n",
        "        dW1, db1, dW2, db2 = nn.backward(X_batch, y_batch, y_pred)\n",
        "\n",
        "        #update params\n",
        "        nn.update_parameters(dW1, db1, dW2, db2, learning_rate)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MnpaiTC7Z_6",
        "outputId": "929ba750-a8c8-489d-c9ee-d9f29c10841c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.2836874127459269\n",
            "Epoch 2, Loss: 0.06612117247419964\n",
            "Epoch 3, Loss: 0.09561253082685671\n",
            "Epoch 4, Loss: 0.1278147574247962\n",
            "Epoch 5, Loss: 0.2264295885635339\n",
            "Epoch 6, Loss: 0.22282814526944886\n",
            "Epoch 7, Loss: 0.12882392352443128\n",
            "Epoch 8, Loss: 0.1763612313337024\n",
            "Epoch 9, Loss: 0.07644822669444895\n",
            "Epoch 10, Loss: 0.032595524572398685\n",
            "Epoch 11, Loss: 0.31113927999823615\n",
            "Epoch 12, Loss: 0.12433621312421594\n",
            "Epoch 13, Loss: 0.10942206097060375\n",
            "Epoch 14, Loss: 0.11798827685939005\n",
            "Epoch 15, Loss: 0.05892724409805021\n",
            "Epoch 16, Loss: 0.4690550749159502\n",
            "Epoch 17, Loss: 0.18159560394302193\n",
            "Epoch 18, Loss: 0.09452276464270279\n",
            "Epoch 19, Loss: 0.41290666803899867\n",
            "Epoch 20, Loss: 0.04236041113067227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test the model\n",
        "y_pred_test = nn.forward(X_test)\n",
        "test_accuracy = np.mean(np.argmax(y_pred_test, axis=1) == y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuTmgfaN7cQw",
        "outputId": "4dcd2740-1d30-45a8-9f88-ebc4501142d1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 95.80%\n"
          ]
        }
      ]
    }
  ]
}